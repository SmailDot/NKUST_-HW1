# 支持向量機 (SVM) 優化過程推導：邊緣最大化的幾何方法

支持向量機 (SVM) 是一種傳統的、基於**幾何原理**的分類模型。它的核心優化目標不是像 MLP 那樣的迭代式梯度下降，而是**一次性尋找一個能最大化邊緣 (Margin) 的最佳超平面**。

## 1. 核心概念：超平面 (Hyperplane)

在 $n$ 維空間中，將數據點區分開的超平面可以由以下線性方程定義：

$$
\mathbf{w} \cdot \mathbf{x} + b = 0
$$

其中：
*   $\mathbf{w}$ 是**法向量**（垂直於超平面）。
*   $b$ 是**截距**。

## 2. 優化目標：最大化邊緣 (Maximum Margin)

SVM 的優化不是尋找一條隨意的線，而是尋找一個距離最近的訓練樣本（即**支持向量**）最遠的超平面。

定義兩條邊緣線：
$$
\mathbf{w} \cdot \mathbf{x} + b = +1 \quad (\text{類別 } +1)
$$
$$
\mathbf{w} \cdot \mathbf{x} + b = -1 \quad (\text{類別 } -1)
$$

這兩條線之間的距離（邊緣寬度 $M$）為：

$$
M = \frac{2}{\|\mathbf{w}\|}
$$

**優化問題：**
為了最大化 $M$，我們需要**最小化 $\|\mathbf{w}\|$**。

## 3. 數學形式：約束優化問題

SVM 的訓練過程（一次性優化）被定義為一個**約束優化問題 (Constrained Optimization Problem)**：

**最小化 (Minimize):**
$$
\frac{1}{2} \|\mathbf{w}\|^2
$$
**滿足約束 (Subject to):**
$$
y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \ge 1 \quad \text{對於所有訓練樣本 } i
$$

*   $y_i$ 是標籤（$+1$ 或 $-1$）。
*   這個約束確保了所有數據點都位於正確的邊緣線之外。

## 4. 解決方案與支持向量

這個優化問題通常通過**拉格朗日乘子法 (Lagrange Multipliers)** 和**二次規劃 (Quadratic Programming)** 來求解。

*   **支持向量 (Support Vectors):** 求解結果中，只有那些使得約束條件等於 $1$ 的數據點 ($\mathbf{w} \cdot \mathbf{x}_i + b = \pm 1$) 才對最終的超平面有貢獻。
*   **優化完成：** 一旦找到 $\mathbf{w}$ 和 $b$ 的最佳解，訓練就**立即結束**，不會有像 MLP 那樣的持續迭代改進。

---
**總結：** SVM 的訓練是透過**一次性**求解一個複雜的**二次規劃問題**來找到最佳的幾何分離平面，其改進目標是**邊緣的最大化**。