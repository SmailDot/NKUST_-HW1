import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time
import numpy as np

# --- å°å…¥ SVM ç›¸é—œåº« ---
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons # ç”¨æ–¼ SVM æ±ºç­–é‚Šç•Œå¯è¦–åŒ–

# --- 1. æ¨¡å‹å®šç¾© (MLP - å¤šå±¤æ„ŸçŸ¥æ©Ÿ) ---
class SimpleNN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 50)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# --- 2. æ•¸æ“šè¼‰å…¥ (å…±ç”¨) ---
@st.cache_data
def load_data():
    """è¼‰å…¥ä¸¦è½‰æ› MNIST æ•¸æ“šé›†ï¼Œä½¿ç”¨ Streamlit cache é¿å…é‡è¤‡ä¸‹è¼‰ã€‚"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    return train_dataset

# --- 3. MLP è¨“ç·´å‡½æ•¸ï¼šæ ¸å¿ƒåœ¨æ–¼å¯¦æ™‚æ›´æ–°å¯è¦–åŒ–æ•¸æ“š (æŒçºŒæ”¹é€²) ---
def train_model_and_visualize(num_epochs, lr, batch_size, placeholder_loss, placeholder_acc):
    
    # è¨­å‚™è¨­å®šï¼šå„ªå…ˆä½¿ç”¨ GPU (CUDA)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    st.info(f"âš¡ï¸ è¨“ç·´è¨­å‚™: {device}. (MLP æ¨¡å‹)")

    train_dataset = load_data()
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ã€æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
    model = SimpleNN(input_size=28*28, num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    loss_history = []
    accuracy_history = []
    
    # è¨­ç½®ç¹ªåœ–å€åŸŸ
    fig_loss, ax_loss = plt.subplots(figsize=(5, 3))
    fig_acc, ax_acc = plt.subplots(figsize=(5, 3))
    
    st.markdown("---")
    st.subheader("ğŸ¤– MLP æ¨¡å‹ (æ·±åº¦å­¸ç¿’) æŒçºŒæ”¹é€²éç¨‹ - å¯¦æ™‚å¯è¦–åŒ–")
    st.markdown("è«‹è§€å¯Ÿï¼š**Loss æ›²ç·š** æ‡‰æŒçºŒ**ä¸‹é™**ï¼›**Accuracy æ›²ç·š** æ‡‰æŒçºŒ**ä¸Šå‡**ã€‚")

    
    # è¨“ç·´å¾ªç’°
    for epoch in range(num_epochs):
        
        epoch_loss = 0
        epoch_correct = 0
        epoch_total = 0

        for i, (images, labels) in enumerate(train_loader):
            
            # æº–å‚™æ•¸æ“šä¸¦ç§»å‹•åˆ°æ­£ç¢ºçš„è¨­å‚™
            images = images.reshape(-1, 28*28).to(device)
            labels = labels.to(device)
            
            # 1. å‰å‘å‚³æ’­ -> 2. è¨ˆç®—æå¤±
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # 3. åå‘å‚³æ’­ -> 4. æ¬Šé‡æ›´æ–° (æŒçºŒæ”¹é€²)
            optimizer.zero_grad() 
            loss.backward()       
            optimizer.step()      

            # çµ±è¨ˆæœ¬è¼ªæ•¸æ“š
            epoch_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs.data, 1)
            epoch_total += labels.size(0)
            epoch_correct += (predicted == labels).sum().item()

            # æ¯éš” 100 æ­¥æ›´æ–°ä¸€æ¬¡å¯è¦–åŒ–
            if (i+1) % 100 == 0:
                loss_history.append(loss.item())
                accuracy_history.append(100 * (predicted == labels).sum().item() / labels.size(0))

                # --- å¯¦æ™‚æ›´æ–°å¯è¦–åŒ– ---
                ax_loss.clear(); ax_loss.plot(loss_history, color='red'); 
                ax_loss.set_title(f'Loss Curve (Epoch {epoch+1})'); ax_loss.set_xlabel('Steps (x100)'); ax_loss.set_ylabel('Loss Value');
                placeholder_loss.pyplot(fig_loss)

                ax_acc.clear(); ax_acc.plot(accuracy_history, color='blue');
                ax_acc.set_title(f'Accuracy Curve (Epoch {epoch+1})'); ax_acc.set_xlabel('Steps (x100)'); ax_acc.set_ylabel('Accuracy (%)');
                ax_acc.set_ylim([np.min(accuracy_history)-5 if accuracy_history and np.min(accuracy_history) > 0 else 80, 100])
                placeholder_acc.pyplot(fig_acc)
                time.sleep(0.01) 
        
        # --- Epoch çµæŸå¾Œçš„ç¸½çµèˆ‡èªªæ˜ (å±•ç¤ºæ”¹é€²çµæœ) ---
        avg_loss = epoch_loss / epoch_total
        avg_accuracy = 100 * epoch_correct / epoch_total
        
        with st.expander(f"âœ… **ç¬¬ {epoch+1} è¼ª MLP è¨“ç·´ç¸½çµèˆ‡åˆ†æ**", expanded=True):
            st.metric(label="æœ¬è¼ªå¹³å‡æå¤± (Loss)", value=f"{avg_loss:.4f}")
            st.metric(label="æœ¬è¼ªå¹³å‡æº–ç¢ºåº¦ (Accuracy)", value=f"{avg_accuracy:.2f}%")
            
            st.markdown("---")
            st.markdown(f"**AI å­¸ç¿’é€²åº¦èªªæ˜ (ç¬¬ {epoch+1} è¼ª):**")
            
            if avg_accuracy < 80:
                 st.warning(f"ç•¶å‰æº–ç¢ºåº¦ä½ï¼ŒAI æ­£åœ¨**åŸºç¤èª¿æ•´**ã€‚")
            elif avg_accuracy < 90:
                st.info(f"ç•¶å‰æº–ç¢ºåº¦ç‚º {avg_accuracy:.2f}%ï¼Œæ¨¡å‹å·²é€²å…¥**ç©©å®šæ”¹é€²éšæ®µ**ã€‚")
            else:
                st.success(f"æ¨¡å‹**æ”¹é€²æ•ˆæœé¡¯è‘—**ã€‚ç¾åœ¨çš„æŒ‘æˆ°æ˜¯ç¶­æŒé«˜æº–ç¢ºåº¦ä¸¦é¿å…éåº¦æ“¬åˆã€‚")

        st.markdown("---")
        
    st.success("MLP è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²åœæ­¢æ”¹é€²ã€‚")


# --- 4. SVM è¨“ç·´å°æ¯”å‡½æ•¸ (ä¸€æ¬¡æ€§å„ªåŒ–) ---

@st.cache_data
def load_data_for_svm():
    """è¼‰å…¥ä¸¦æº–å‚™é©åˆ SVM çš„æ•¸æ“š (MNIST æ•¸æ“šé›†)"""
    train_dataset = datasets.MNIST('./data', train=True, download=True, 
                                   transform=transforms.Compose([transforms.ToTensor()]))
    
    # è½‰æ›ç‚º numpy æ•¸çµ„ï¼Œä¸¦é™åˆ¶æ•¸æ“šé‡ï¼Œå› ç‚º SVM è¨“ç·´é€Ÿåº¦è¼ƒæ…¢
    X = train_dataset.data.numpy().reshape(-1, 28*28)[:5000] # åªå– 5000 æ¨£æœ¬
    y = train_dataset.targets.numpy()[:5000]
    
    X = X / 255.0 # æ¨™æº–åŒ–æ•¸æ“š
    
    return train_test_split(X, y, test_size=0.2, random_state=42)

def run_svm_comparison(C_param, gamma_param):
    """è¨“ç·´ SVM ä¸¦è¨ˆç®—æº–ç¢ºåº¦"""
    
    X_train, X_test, y_train, y_test = load_data_for_svm()
    
    st.markdown("---")
    st.subheader("ğŸ“Š SVM æ¨¡å‹ (å‚³çµ±åˆ†é¡) è¨“ç·´çµæœ")
    
    try:
        svm_model = SVC(C=C_param, gamma=gamma_param, kernel='rbf', verbose=False)
        
        svm_status = st.empty()
        svm_status.info("æ­£åœ¨è¨“ç·´ SVM æ¨¡å‹ (é€™æ˜¯**ä¸€æ¬¡æ€§å„ªåŒ–**éç¨‹ï¼Œè«‹ç¨å€™...)")
        
        start_time = time.time()
        svm_model.fit(X_train, y_train) 
        end_time = time.time()

        svm_status.success(f"SVM è¨“ç·´å®Œæˆï¼è€—æ™‚: {end_time - start_time:.2f} ç§’")

        y_pred = svm_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        st.metric(label="SVM æœ€çµ‚æº–ç¢ºåº¦ (åœ¨æ¸¬è©¦é›†ä¸Š)", value=f"{accuracy * 100:.2f}%")
        st.markdown("**å­¸ç¿’æ©Ÿåˆ¶èªªæ˜ï¼š** SVM çš„ç›®æ¨™æ˜¯æœ€å¤§åŒ–æ±ºç­–é‚Šç·£ï¼Œé€šå¸¸é€é**äºŒæ¬¡è¦åŠƒ**ä¾†æ±‚è§£ï¼Œè€Œä¸æ˜¯åƒ MLP é‚£æ¨£é€²è¡ŒæŒçºŒçš„æ¢¯åº¦è¿­ä»£ã€‚å› æ­¤ï¼Œå…¶çµæœæ˜¯å›ºå®šçš„ã€‚")
        
    except Exception as e:
        st.error(f"SVM è¨“ç·´å‡ºéŒ¯: {e}")

# --- 5. SVM æ±ºç­–é‚Šç•Œå¯è¦–åŒ–å‡½æ•¸ (å¹¾ä½•å„ªåŒ–) ---
def visualize_svm_boundary(C_param, gamma_param):
    """å¯è¦–åŒ–ä½ç¶­åº¦æ•¸æ“šé›†ä¸Š SVM æ±ºç­–é‚Šç•Œçš„å½¢æˆ (éå¯¦æ™‚è¿­ä»£)"""
    
    st.markdown("---")
    st.subheader("ğŸ–¼ï¸ SVM æ±ºç­–é‚Šç•Œå¯è¦–åŒ– (ä½ç¶­åº¦æ¨¡æ“¬)")
    st.markdown("ç‚ºäº†å¯è¦–åŒ–ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹**äºŒç¶­æ¨¡æ“¬æ•¸æ“šé›†**ã€‚SVM çš„å„ªåŒ–ç›®æ¨™ï¼š**æœ€å¤§åŒ–æ±ºç­–é‚Šç·£ (Margin)**ï¼Œä¸¦è­˜åˆ¥**æ”¯æŒå‘é‡**ã€‚")
    
    # å‰µå»ºä¸€å€‹éç·šæ€§å¯åˆ†çš„æ¨¡æ“¬æ•¸æ“šé›† (å…©å€‹æœˆç‰™å½¢)
    X, y = make_moons(n_samples=200, noise=0.1, random_state=42)
    
    # è¨“ç·´ SVM
    svm_model = SVC(C=C_param, gamma=gamma_param, kernel='rbf')
    
    status_placeholder = st.empty()
    status_placeholder.info("æ­£åœ¨è¨“ç·´ SVM ä¸¦ç¹ªè£½æ±ºç­–é‚Šç•Œ (ä¸€æ¬¡æ€§å¹¾ä½•å„ªåŒ–)")
    
    svm_model.fit(X, y)
    status_placeholder.success("SVM å„ªåŒ–å®Œæˆï¼æ±ºç­–é‚Šç•Œå·²æ‰¾åˆ°ã€‚")

    # --- ç¹ªè£½æ±ºç­–é‚Šç•Œ ---
    
    # è¨­ç½®ç¶²æ ¼ç¯„åœ
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    
    # èª¿æ•´ç¶²æ ¼å¯†åº¦ç‚º 0.01 (æ›´ç´°ç·»)
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))
    
    # é æ¸¬ç¶²æ ¼ä¸Šçš„é»ï¼Œä¸¦ç²å–è·é›¢ (Distance) ä»¥ç¹ªè£½é‚Šç·£
    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # ç²å–è·é›¢ï¼šç”¨æ–¼ç¹ªè£½é‚Šç·£ (Margin)
    # decision_function è¿”å›æ¯å€‹é»åˆ°æ±ºç­–é‚Šç•Œ Signed Distance
    W = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    # èª¿æ•´ figsize (é•·å¯¬æ¯”)
    fig, ax = plt.subplots(figsize=(6, 5)) 
    
    # ç¹ªè£½èƒŒæ™¯é¡è‰² (æ±ºç­–å€åŸŸ)
    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    
    # ç¹ªè£½æ±ºç­–é‚Šç•Œ (W=0) å’Œé‚Šç·£ (W=1 å’Œ W=-1)
    ax.contour(xx, yy, W, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], 
               levels=[-1, 0, 1])
    
    # ç¹ªè£½æ•¸æ“šé»
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')
    
    # ç¹ªè£½æ”¯æŒå‘é‡ (Support Vectors)
    sv = svm_model.support_vectors_
    ax.scatter(sv[:, 0], sv[:, 1], s=150, facecolors='none', edgecolors='green', linewidths=1.5, label='Support Vectors')
    
    ax.set_title(f'SVM æ±ºç­–é‚Šç•Œ (C={C_param:.2f}, Gamma={gamma_param:.3f})')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.legend()
    
    # é—œéµï¼šä½¿ç”¨ use_container_width=True è®“åœ–è¡¨é©æ‡‰ Streamlit æ¬„ä½å¯¬åº¦
    st.pyplot(fig, use_container_width=True) 
    
    st.markdown("---")
    st.markdown("#### åœ–è¡¨è§£è®€ (SVM çš„ã€Œå„ªåŒ–ã€):")
    st.markdown("1. **å¯¦ç·š (Margin=0, é»‘ç·š)ï¼š** é€™æ˜¯æœ€çµ‚çš„**æ±ºç­–é‚Šç•Œ**ã€‚å®ƒä½æ–¼å…©æ¢è™›ç·šçš„æ­£ä¸­å¤®ã€‚")
    st.markdown("2. **è™›ç·š (Margin=Â±1, é»‘è™›ç·š)ï¼š** å…©æ¢è™›ç·šä¹‹é–“çš„å€åŸŸå°±æ˜¯ SVM è©¦åœ–**æœ€å¤§åŒ–**çš„**æ±ºç­–é‚Šç·£ (Margin)**ã€‚")
    st.markdown("3. **æ”¯æŒå‘é‡ (ç¶ è‰²åœ“åœˆ)ï¼š** é€™äº›é»æ˜¯**å”¯ä¸€**æ±ºå®šæ±ºç­–é‚Šç•Œå’Œé‚Šç·£ä½ç½®çš„æ•¸æ“šé»ã€‚SVM çš„å„ªåŒ–ç›®æ¨™æ˜¯ï¼š**æ‰¾åˆ°ä¸€æ¢é‚Šç•Œï¼Œè®“ç¶ è‰²åœ“åœˆåˆ°å®ƒçš„è·é›¢æœ€å¤§åŒ–ã€‚**")
    st.markdown(f"4. **æ¨¡å‹ç©©å®šæ€§ï¼š** SVM é¸æ“‡çš„é‚Šç•Œ (é»‘å¯¦ç·š) ä¸æœƒåƒæ‚¨ç•«çš„ç´…ç·šé‚£æ¨£ç·Šè²¼æ•¸æ“šé»ï¼Œé€™æ˜¯å› ç‚º SVM è¿½æ±‚**ç©©å®šæ€§ (é‚Šç·£æœ€å¤§åŒ–)**ï¼Œé€™é€šå¸¸æ¯”è¿½æ±‚è¨“ç·´é›† 100% æº–ç¢ºåº¦æ›´é‡è¦ã€‚")
# --- 6. Streamlit UI ç•Œé¢ ---
def main():
    st.set_page_config(layout="wide")
    st.title("ğŸ§  æ©Ÿå™¨å­¸ç¿’éç¨‹å¯è¦–åŒ–ï¼šæŒçºŒæ”¹é€² (MLP) vs. ä¸€æ¬¡å„ªåŒ– (SVM)")
    st.markdown("---")
    
    # --- å´é‚Šæ¬„æ§åˆ¶é … ---
    st.sidebar.title("åƒæ•¸æ§åˆ¶ä¸­å¿ƒ")
    
    # MLP åƒæ•¸è¨­å®š
    st.sidebar.header("âš™ï¸ MLP è¨“ç·´åƒæ•¸ (æŒçºŒæ”¹é€²)")
    num_epochs = st.sidebar.slider("è¨“ç·´è¼ªæ•¸ (Epochs)", 1, 10, 3, key='mlp_epochs')
    with st.sidebar.expander("â“ è¨“ç·´è¼ªæ•¸ (Epochs) èªªæ˜"):
        st.markdown("å®šç¾©æ¨¡å‹å®Œæ•´åœ°æƒæä¸€éæ‰€æœ‰è¨“ç·´æ•¸æ“šçš„æ¬¡æ•¸ã€‚")
    
    lr = st.sidebar.slider("å­¸ç¿’ç‡ (Learning Rate)", 0.001, 0.1, 0.01, format="%f", key='mlp_lr')
    with st.sidebar.expander("â“ å­¸ç¿’ç‡ (Learning Rate) èªªæ˜"):
        st.markdown("å®šç¾©æ¯æ¬¡åƒæ•¸èª¿æ•´ï¼ˆæ”¹é€²ï¼‰çš„æ­¥é•·ã€‚")
    
    batch_size = st.sidebar.slider("æ‰¹é‡å¤§å° (Batch Size)", 32, 256, 128, key='mlp_batch')
    with st.sidebar.expander("â“ æ‰¹é‡å¤§å° (Batch Size) èªªæ˜"):
        st.markdown("å®šç¾©æ¯æ¬¡è¨ˆç®—æ¢¯åº¦å’Œèª¿æ•´æ¬Šé‡æ™‚æ‰€ä½¿ç”¨çš„æ•¸æ“šé‡ã€‚")

    st.sidebar.markdown("---")
    
    # SVM åƒæ•¸è¨­å®š
    st.sidebar.header("âš™ï¸ SVM æ¨¡å‹åƒæ•¸ (ä¸€æ¬¡å„ªåŒ–)")
    C_param = st.sidebar.slider("SVM C åƒæ•¸ (æ­£å‰‡åŒ–)", 0.1, 10.0, 1.0, format="%f", key='svm_c')
    with st.sidebar.expander("â“ C åƒæ•¸èªªæ˜"):
        st.markdown("C åƒæ•¸æ±ºå®šäº†å°éŒ¯èª¤åˆ†é¡æ¨£æœ¬çš„æ‡²ç½°ç¨‹åº¦ã€‚C è¶Šé«˜ï¼Œæ¨¡å‹è¶Šè¤‡é›œï¼Œé‚Šç·£è¶Šçª„ã€‚")
        
    gamma_param = st.sidebar.slider("SVM Gamma åƒæ•¸ (æ ¸å‡½æ•¸å½±éŸ¿ç¯„åœ)", 0.001, 0.1, 0.01, format="%f", key='svm_gamma')
    with st.sidebar.expander("â“ Gamma åƒæ•¸èªªæ˜"):
        st.markdown("Gamma å®šç¾©äº†å–®å€‹è¨“ç·´æ¨£æœ¬çš„å½±éŸ¿ç¯„åœã€‚Gamma è¶Šé«˜ï¼Œå½±éŸ¿ç¯„åœè¶Šå°ï¼Œæ¨¡å‹å¯èƒ½éåº¦æ“¬åˆã€‚")


    # --- åŸ·è¡ŒæŒ‰éˆ• ---
    st.sidebar.markdown("---")
    
    st.subheader("ğŸ’¡ é¸æ“‡æ‚¨æƒ³åŸ·è¡Œçš„å‹•ä½œï¼š")
    
    col_btn_mlp, col_btn_svm, col_btn_svm_viz = st.columns(3)
    
    if col_btn_mlp.button("ğŸš€ é–‹å§‹ MLP è¨“ç·´ (æŒçºŒæ”¹é€²)"):
        # å‰µå»ºå…©æ¬„ç”¨æ–¼ MLP å¯¦æ™‚è¨“ç·´åœ–è¡¨
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ“‰ MLP æå¤±æ›²ç·š (Loss Curve)")
            placeholder_loss = st.empty()
            
        with col2:
            st.markdown("#### ğŸ“ˆ MLP æº–ç¢ºåº¦æ›²ç·š (Accuracy Curve)")
            placeholder_acc = st.empty()
        
        train_model_and_visualize(num_epochs, lr, batch_size, placeholder_loss, placeholder_acc)

    if col_btn_svm.button("ğŸ”¬ é‹è¡Œ SVM æº–ç¢ºåº¦å°æ¯”"):
        run_svm_comparison(C_param, gamma_param)

    if col_btn_svm_viz.button("ğŸ–¼ï¸ SVM æ±ºç­–é‚Šç•Œå¯è¦–åŒ–"):
        visualize_svm_boundary(C_param, gamma_param)

if __name__ == "__main__":
    main()