# 支持向量機 (SVM) 理論推導與實作說明

以專案中的 `svm.py` 為例，推倒如何以**次梯度下降法 (Subgradient Descent)** 求得最優解。
---

## 1. 問題定義：最大化間隔的分類器

給定訓練資料集
$${(x_i, y_i)}_{i=1}^{N}, \quad x_i \in \mathbb{R}^d, \ y_i \in {-1, +1}$$
SVM 的目標是找到一個分類超平面：

$$
f(x) = \text{sign}(w^T x + b)
$$

使得不同類別的樣本被最大化分隔。
這可轉化為以下最佳化問題：

$$
\begin{aligned}
\min_{w, b, \xi} \quad & \frac{1}{2}|w|^2 + C \sum_{i=1}^N \xi_i \
\text{s.t.} \quad & y_i (w^T x_i + b) \ge 1 - \xi_i, \
& \xi_i \ge 0, \ \forall i
\end{aligned}
$$

> * $\frac{1}{2}|w|^2$：控制邊界的平滑度（希望 $w$ 不過大）。
> * $\xi_i$：允許部分資料點違反邊界。
> * $C$：懲罰係數，控制「誤差」與「間隔」的權衡。

---

## 2. 目標函數：鉸鏈損失 (Hinge Loss)

上式可轉寫為無約束形式：

$$
J(w, b) = \frac{1}{2}|w|^2 + C \sum_{i=1}^N \max(0, 1 - y_i (w^T x_i + b))
$$

其中第二項 $\max(0, 1 - y_i (w^T x_i + b))$ 即為 **Hinge Loss**，
當資料點分類正確且距離邊界超過 1 時，損失為 0；否則損失線性增長。

---

## 3. 次梯度 (Subgradient) 的推導

由於 Hinge Loss 在 `1 - y_i (w^T x_i + b) = 0` 處不可微，使用**次梯度 (Subgradient)** 的概念進行更新。

對於每一筆樣本，根據條件分段求導：

| 條件                                                | 損失狀態      | 次梯度 ∇<sub>w</sub>J(w,b)              |
| :------------------------------------------------ | :-------- | :----------------------------------- |
| y<sub>i</sub>(w<sup>T</sup>x<sub>i</sub> + b) ≥ 1 | 正確分類且距離足夠 | **w**                                |
| y<sub>i</sub>(w<sup>T</sup>x<sub>i</sub> + b) < 1 | 位於間隔內或誤判  | **w - C·y<sub>i</sub>x<sub>i</sub>** |

對偏置 b 的梯度同理：

```
∇_b J = {
    0,          若 y_i (w^T x_i + b) ≥ 1
    -C·y_i,     若 y_i (w^T x_i + b) < 1
}
```

---

## 4. 次梯度下降法 (SGD) 實作對應

`svm.py` 以隨機梯度下降 (SGD) 的形式逐筆樣本更新參數：

```python
if decision >= 1:
    dw = w
    db = 0
else:
    dw = w - C * yi * xi
    db = -C * yi
```

對應到更新公式：

```
w := w - η · ∇_w J
b := b - η · ∇_b J
```

程式中即為：

```python
w -= learning_rate * dw
b -= learning_rate * db
```

---

## 5. 幾何意義：支持向量的角色

經過多次迭代後，只有少數樣本點真正影響最終的 $w$：

| 樣本狀態                    | 幾何位置   | 是否影響 $w$ |
| :---------------------- | :----- | :------- |
| $y_i (w^T x_i + b) > 1$ | 邊界外    | ❌ 無影響    |
| $y_i (w^T x_i + b) = 1$ | 邊界上    | ✅ 支持向量   |
| $y_i (w^T x_i + b) < 1$ | 邊界內或誤判 | ✅ 被懲罰樣本  |

因此，SVM 的「支持向量 (Support Vectors)」即是那些
位於間隔邊界或違反間隔的關鍵樣本。

---

## 6. Python 實作結果

執行 `svm.py`：

```bash
PS E:\SVM_MLP_traing> python .\svm.py
SVM 訓練完成！
最終權重 W: [-1.4312307   1.82241949]  
最終偏置 b: -1.7000000000000004
```

此輸出對應於模型最終學得的決策超平面：
```
f(x) = sign(w^T x + b)
```

---

## ✅ 小結

| 概念       | 說明                                                                   |
| :------- | :------------------------------------------------------------------- |
| **損失函數** | J(w, b) = ½ · ‖w‖² + C · Σ max(0, 1 − yᵢ(wᵀxᵢ + b)) |
| **核心思想** | 以最大化間隔為目標，同時允許部分違反樣本                                                 |
| **數值方法** | 使用次梯度下降法逐步更新 $w, b$                                                  |
| **關鍵樣本** | 僅支持向量 ($y_i (w^T x_i + b) = 1$) 影響最終模型                               |
| **凸性特性** | 此為凸優化問題，理論上可收斂至全域最優解                                                 |

---
